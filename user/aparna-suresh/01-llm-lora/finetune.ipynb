{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1313cd8f",
   "metadata": {},
   "source": [
    "# LLM LoRA fine-tuning\n",
    "\n",
    "⚠️ **IMPORTANT:** Before running this notebook, copy it into your user directory at `user/<your-username>/`.  \n",
    "This ensures that any output or intermediate checkpoints are stored in your personal workspace.\n",
    "\n",
    "Once copied, follow the environment setup instructions in the [README.md](./README.md), and connect this notebook to the `.venv` environment you created during setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4283d92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hcoda1/6/asuresh85/dsgt-arc/fall-2025-interest-group-projects/project/01-llm-lora/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from finetune import (\n",
    "    CONFIGS,\n",
    "    get_device,\n",
    "    load_and_prepare_dataset,\n",
    "    tokenize_dataset,\n",
    "    create_lora_model,\n",
    "    run_experiment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66142a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LoRA fine-tuning on rotten_tomatoes (roberta-base)\n"
     ]
    }
   ],
   "source": [
    "# choose config\n",
    "config = CONFIGS[\"quick\"]  # or \"full\"\n",
    "device = get_device()\n",
    "print(f\"Running LoRA fine-tuning on {config['dataset_name']} ({config['model_name']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ca0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and tokenize data\n",
    "dataset, text_field = load_and_prepare_dataset(config[\"dataset_name\"])\n",
    "tokenized_dataset, tokenizer = tokenize_dataset(\n",
    "    dataset, text_field, config[\"model_name\"], config[\"max_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3946f680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/1602 01:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.337258</td>\n",
       "      <td>0.863039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.317991</td>\n",
       "      <td>0.873358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.323877</td>\n",
       "      <td>0.877111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LoRA Fine-Tuning Results ===\n",
      "experiment_name     : LoRA Fine-Tuning\n",
      "training_time       : 80.67294120788574\n",
      "train_loss          : 0.34326045652304993\n",
      "eval_loss           : 0.32387739419937134\n",
      "eval_accuracy       : 0.8771106941838649\n",
      "total_params        : 126248795\n",
      "trainable_params    : 1603163\n",
      "trainable_percentage: 1.269844199305031\n"
     ]
    }
   ],
   "source": [
    "# create and train LoRA model\n",
    "lora_alpha = config[\"lora_rank\"] * config[\"lora_alpha_ratio\"]\n",
    "model = create_lora_model(\n",
    "    config[\"model_name\"],\n",
    "    config[\"num_labels\"],\n",
    "    rank=config[\"lora_rank\"],\n",
    "    alpha=lora_alpha,\n",
    ")\n",
    "\n",
    "results, model = run_experiment(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    tokenized_dataset=tokenized_dataset,\n",
    "    config=config,\n",
    "    experiment_name=\"LoRA Fine-Tuning\",\n",
    "    learning_rate=config[\"learning_rate_lora\"],\n",
    "    is_full_finetuning=False,\n",
    ")\n",
    "\n",
    "print(\"\\n=== LoRA Fine-Tuning Results ===\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k:20s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d8e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/1602 00:51 < 00:25, 20.66 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>0.328698</td>\n",
       "      <td>0.881801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.377371</td>\n",
       "      <td>0.878049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 16, 'learning_rate_lora': 0.000613240754004354, 'lora_alpha_ratio': 2, 'lora_rank': 16, 'num_epochs': 3}\n",
      "  7%|▋         | 1/15 [00:54<12:36, 54.02s/trial, best loss: -0.8780487804878049]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2134' max='3201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2134/3201 01:38 < 00:49, 21.74 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.355816</td>\n",
       "      <td>0.867730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.402242</td>\n",
       "      <td>0.881801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 8, 'learning_rate_lora': 0.00046681341704344183, 'lora_alpha_ratio': 1, 'lora_rank': 32, 'num_epochs': 3}\n",
      " 13%|█▎        | 2/15 [02:34<17:40, 81.58s/trial, best loss: -0.8818011257035647]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 534/1068 00:36 < 00:36, 14.59 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.338959</td>\n",
       "      <td>0.866792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.344483</td>\n",
       "      <td>0.878049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 32, 'learning_rate_lora': 0.0008119975439233064, 'lora_alpha_ratio': 4, 'lora_rank': 32, 'num_epochs': 4}\n",
      " 20%|██        | 3/15 [03:13<12:24, 62.01s/trial, best loss: -0.8818011257035647]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='2670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/2670 01:18 < 00:52, 20.27 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.342575</td>\n",
       "      <td>0.870544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.313006</td>\n",
       "      <td>0.886492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.372446</td>\n",
       "      <td>0.888368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 16, 'learning_rate_lora': 0.0006359526416258581, 'lora_alpha_ratio': 1, 'lora_rank': 32, 'num_epochs': 5}\n",
      " 27%|██▋       | 4/15 [04:34<12:46, 69.64s/trial, best loss: -0.8883677298311444]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2134' max='5335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2134/5335 01:38 < 02:27, 21.73 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.335947</td>\n",
       "      <td>0.885553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.432739</td>\n",
       "      <td>0.876173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 8, 'learning_rate_lora': 0.0004196840225323575, 'lora_alpha_ratio': 4, 'lora_rank': 8, 'num_epochs': 5}\n",
      " 33%|███▎      | 5/15 [06:15<13:29, 80.92s/trial, best loss: -0.8883677298311444]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2134' max='5335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2134/5335 01:38 < 02:28, 21.55 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.340700</td>\n",
       "      <td>0.338558</td>\n",
       "      <td>0.859287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.496633</td>\n",
       "      <td>0.883677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 8, 'learning_rate_lora': 0.0008968641488420485, 'lora_alpha_ratio': 4, 'lora_rank': 4, 'num_epochs': 5}\n",
      " 40%|████      | 6/15 [07:57<13:12, 88.01s/trial, best loss: -0.8883677298311444]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='2670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/2670 00:52 < 01:18, 20.49 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.341564</td>\n",
       "      <td>0.872420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271800</td>\n",
       "      <td>0.368662</td>\n",
       "      <td>0.876173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 16, 'learning_rate_lora': 0.0009256616285396669, 'lora_alpha_ratio': 2, 'lora_rank': 8, 'num_epochs': 5}\n",
      " 47%|████▋     | 7/15 [08:52<10:16, 77.07s/trial, best loss: -0.8883677298311444]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1602' max='2670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/2670 01:25 < 00:57, 18.72 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.309100</td>\n",
       "      <td>0.321340</td>\n",
       "      <td>0.866792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.275900</td>\n",
       "      <td>0.309848</td>\n",
       "      <td>0.875235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.344953</td>\n",
       "      <td>0.884615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 16, 'learning_rate_lora': 0.0005224642446362386, 'lora_alpha_ratio': 1, 'lora_rank': 8, 'num_epochs': 5}\n",
      " 53%|█████▎    | 8/15 [10:20<09:23, 80.49s/trial, best loss: -0.8883677298311444]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4268' max='4268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4268/4268 03:20, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.386101</td>\n",
       "      <td>0.870544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.299700</td>\n",
       "      <td>0.357402</td>\n",
       "      <td>0.887430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.342198</td>\n",
       "      <td>0.890244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.348691</td>\n",
       "      <td>0.889306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 8, 'learning_rate_lora': 0.00048117556815255064, 'lora_alpha_ratio': 1, 'lora_rank': 2, 'num_epochs': 4}\n",
      " 60%|██████    | 9/15 [13:43<11:52, 118.81s/trial, best loss: -0.8893058161350844]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3201' max='3201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3201/3201 02:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.307800</td>\n",
       "      <td>0.413896</td>\n",
       "      <td>0.857411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.374555</td>\n",
       "      <td>0.887430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>0.367695</td>\n",
       "      <td>0.889306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 8, 'learning_rate_lora': 0.0005675936279443396, 'lora_alpha_ratio': 2, 'lora_rank': 4, 'num_epochs': 3}\n",
      " 67%|██████▋   | 10/15 [16:11<10:40, 128.05s/trial, best loss: -0.8893058161350844]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 801/1068 00:55 < 00:18, 14.27 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.346300</td>\n",
       "      <td>0.374416</td>\n",
       "      <td>0.843340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.327928</td>\n",
       "      <td>0.882739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.371163</td>\n",
       "      <td>0.888368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 32, 'learning_rate_lora': 0.000657554401165412, 'lora_alpha_ratio': 1, 'lora_rank': 64, 'num_epochs': 4}\n",
      " 73%|███████▎  | 11/15 [17:10<07:06, 106.70s/trial, best loss: -0.8893058161350844]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2134' max='4268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2134/4268 01:44 < 01:44, 20.46 it/s, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.344100</td>\n",
       "      <td>0.361161</td>\n",
       "      <td>0.878049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.307700</td>\n",
       "      <td>0.375145</td>\n",
       "      <td>0.883677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 8, 'learning_rate_lora': 0.0005212542251038623, 'lora_alpha_ratio': 3, 'lora_rank': 2, 'num_epochs': 4}\n",
      " 80%|████████  | 12/15 [18:57<05:20, 106.80s/trial, best loss: -0.8893058161350844]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3201' max='3201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3201/3201 02:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.405723</td>\n",
       "      <td>0.866792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.381790</td>\n",
       "      <td>0.891182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.195900</td>\n",
       "      <td>0.392478</td>\n",
       "      <td>0.893058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 8, 'learning_rate_lora': 0.0005067939505918023, 'lora_alpha_ratio': 2, 'lora_rank': 8, 'num_epochs': 3}\n",
      " 87%|████████▋ | 13/15 [21:34<04:04, 122.06s/trial, best loss: -0.8930581613508443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/1602 00:53 < 00:26, 20.09 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.328400</td>\n",
       "      <td>0.308686</td>\n",
       "      <td>0.873358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.381112</td>\n",
       "      <td>0.878987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 16, 'learning_rate_lora': 0.0006919976048763512, 'lora_alpha_ratio': 1, 'lora_rank': 64, 'num_epochs': 3}\n",
      " 93%|█████████▎| 14/15 [22:30<01:42, 102.10s/trial, best loss: -0.8930581613508443]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 801/1068 00:56 < 00:18, 14.22 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.350343</td>\n",
       "      <td>0.848030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.304159</td>\n",
       "      <td>0.887430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.325938</td>\n",
       "      <td>0.894934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'batch_size': 32, 'learning_rate_lora': 0.000744184792707988, 'lora_alpha_ratio': 3, 'lora_rank': 4, 'num_epochs': 4}\n",
      "100%|██████████| 15/15 [23:28<00:00, 93.93s/trial, best loss: -0.8949343339587242] \n",
      "Actual best_params = {'batch_size': 64, 'learning_rate_lora': 0.000744184792707988, 'lora_alpha_ratio': 3, 'lora_rank': 16, 'num_epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "# region\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "def tune_model(config_params):\n",
    "    for k,v in config_params.items():\n",
    "        config[k] = v\n",
    "    lora_alpha = config[\"lora_rank\"] * config[\"lora_alpha_ratio\"]\n",
    "    model = create_lora_model(\n",
    "        config[\"model_name\"],\n",
    "        config[\"num_labels\"],\n",
    "        rank=config[\"lora_rank\"],\n",
    "        alpha=lora_alpha,\n",
    "    )\n",
    "\n",
    "    results, model = run_experiment(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        tokenized_dataset=tokenized_dataset,\n",
    "        config=config,\n",
    "        experiment_name=\"LoRA Fine-Tuning\",\n",
    "        learning_rate=config[\"learning_rate_lora\"],\n",
    "        is_full_finetuning=False,\n",
    "    )\n",
    "\n",
    "   \n",
    "    print(f\"params ======== {config_params}\")\n",
    "    print(f\"\\n=== LoRA Fine-Tuning Results ===\")\n",
    "    for k, v in results.items():\n",
    "        print(f\"{k:20s}: {v}\")\n",
    "    return {\"loss\" : -(results[\"eval_accuracy\"]), \"accuracy\": results[\"eval_accuracy\"], \"status\": STATUS_OK}\n",
    "\n",
    "search_space = {\n",
    "    \"lora_rank\" : hp.choice(\"lora_rank\",[8, 16, 32, 64, 128]),\n",
    "    \"lora_alpha_ratio\" : hp.choice(\"lora_alpha_ratio\", [2,3,4,5,6,7,8]),\n",
    "    \"learning_rate_lora\" : hp.uniform(\"learning_rate_lora\", 1e-5,  1e-3),\n",
    "    \"num_epochs\" : hp.choice(\"num_epochs\",[2,3,4,5]),\n",
    "    \"batch_size\" : hp.choice(\"batch_size\",[16,32,64,128]),\n",
    "}\n",
    "\n",
    "search_space_2 = {\n",
    "    \"lora_rank\" : hp.choice(\"lora_rank\",[8, 16, 32,48, 64]),\n",
    "    \"lora_alpha_ratio\" : hp.choice(\"lora_alpha_ratio\", [1, 2, 3, 4]),\n",
    "    \"learning_rate_lora\" : hp.uniform(\"learning_rate_lora\", 4e-4,  1e-3),\n",
    "    \"num_epochs\" : hp.choice(\"num_epochs\",[3, 4, 5]),\n",
    "    \"batch_size\" : hp.choice(\"batch_size\",[8,16,32,64]),\n",
    "}\n",
    "trials = Trials()\n",
    "best_params = fmin(\n",
    "    fn=tune_model,\n",
    "    # space=search_space,\n",
    "    space=search_space_2,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=15,\n",
    "    trials=trials,\n",
    "    verbose=True,\n",
    ")\n",
    "# Find and log best results\n",
    "best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n",
    "actual_best_params = space_eval(search_space, best_params)\n",
    "print(f\"Actual best_params = {actual_best_params}\")\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afabb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "There are adapters available but none are activated for the forward pass.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='536' max='536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [536/536 01:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.335308</td>\n",
       "      <td>0.860225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.306515</td>\n",
       "      <td>0.883677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.197200</td>\n",
       "      <td>0.294156</td>\n",
       "      <td>0.891182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.330841</td>\n",
       "      <td>0.890244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params ======== {'dataset_name': 'rotten_tomatoes', 'model_name': 'roberta-base', 'max_length': 128, 'num_labels': 2, 'num_epochs': 4, 'batch_size': 64, 'learning_rate_full': 2e-05, 'learning_rate_lora': 0.00074418, 'lora_rank': 16, 'lora_alpha_ratio': 3, 'description': 'Quick experiment: Rotten Tomatoes + RoBERTa-base'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': -0.8902439024390244, 'accuracy': 0.8902439024390244, 'status': 'ok'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"batch_size\"]=64\n",
    "config[\"learning_rate_lora\"] = 0.00074418\n",
    "config['lora_alpha_ratio'] = 3\n",
    "config[\"lora_rank\"] = 16\n",
    "config['num_epochs'] = 3\n",
    "tune_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
